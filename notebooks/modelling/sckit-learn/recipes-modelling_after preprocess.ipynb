{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\arsen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import bigrams \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/arsen/Healthylicious/data/cleaned/csv/recipes_cleaned_with_ids.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv(\"C:/Users/arsen\\Healthylicious/data/cleaned/just ingredients/just_ingredients.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def commatokenizer(text):\n",
    "    return text.split(', ')\n",
    "\n",
    "def get_nouns(text):\n",
    "    tokens = RegexpTokenizer(r'\\w+').tokenize(text)\n",
    "    nouns = {'NN', 'NNS', 'NNP', 'NNPS', 'NOUN', 'PROPN', 'NE', 'NNE', 'NR'}\n",
    "    nounlist = [token for token in tokens if nlp(token)[0].tag_ in nouns]\n",
    "    return ', '.join(nounlist)\n",
    "\n",
    "def mytokenizer(combinedlist):\n",
    "    ingredlist = combinedlist[0].split(', ')\n",
    "    nounlist = combinedlist[1].split(', ')\n",
    "    bigramlist = []\n",
    "    for ingred in ingredlist:\n",
    "        bigrms = [bi for bi in bigrams(ingred.split())]\n",
    "        for bi in bigrms:\n",
    "            if (bi[0] in nounlist) or (bi[1] in nounlist):\n",
    "                bigramlist.append(' '.join((bi[0], bi[1])))\n",
    "    return ', '.join(bigramlist + nounlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ingredients(row):\n",
    "    nouns = get_nouns(row)\n",
    "    combined = [row, nouns]\n",
    "    return mytokenizer(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['TokenizedIngredients'] = df1['IngredientsRemovedAdj'].apply(process_ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_tokenize(ingreds):\n",
    "    nouns = get_nouns(ingreds)\n",
    "    ingredscombined = [ingreds, nouns]\n",
    "    ingredstokenized = mytokenizer(ingredscombined)\n",
    "    return ingredstokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1097, 249)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arsen\\Healthylicious\\.venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=commatokenizer, stop_words='english', min_df=7, max_df=0.4)\n",
    "docs = df1['TokenizedIngredients']\n",
    "doc_word = vectorizer.fit_transform(docs)\n",
    "\n",
    "print(doc_word.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(20, random_state=10, max_iter=1000)\n",
    "doc_topic = nmf_model.fit_transform(doc_word)\n",
    "topic_word = nmf_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "brown, brown sugar, sugar, cinnamon, oat, butter, pecan, vanilla\n",
      "\n",
      "Topic  1\n",
      "pepper, black pepper, garlic, butter, cayenne pepper, cayenne, onion, parsley\n",
      "\n",
      "Topic  2\n",
      "stone, stone house, house, house seasoning, parsley, ranch, bread, chili powder\n",
      "\n",
      "Topic  3\n",
      "cheddar, cheddar cheese, cheese, bacon, mayonnaise, ham, green onion, milk\n",
      "\n",
      "Topic  4\n",
      "olive, olive oil, oil, zucchini, pork, shrimp, garlic, pesto\n",
      "\n",
      "Topic  5\n",
      "powder, baking powder, flour, sugar, butter, vanilla, vanilla extract, cinnamon\n",
      "\n",
      "Topic  6\n",
      "vanilla, vanilla extract, milk, sugar, cocoa, butter, espresso, condensed milk\n",
      "\n",
      "Topic  7\n",
      "chicken, stock, chicken stock, broth, soup, thyme, celery, mushroom\n",
      "\n",
      "Topic  8\n",
      "water, yeast, corn, ice, sugar, corn syrup, rice, vegetable\n",
      "\n",
      "Topic  9\n",
      "peanut, peanut butter, butter, oat, chocolate chip, honey, chip, vanilla ice\n",
      "\n",
      "Topic  10\n",
      "self, self rising, rising flour, flour, milk, butter, cornmeal, yeast\n",
      "\n",
      "Topic  11\n",
      "cream, cream cheese, cheese, sour cream, whipping cream, ice cream, ice, vanilla ice\n",
      "\n",
      "Topic  12\n",
      "chocolate, candy, espresso, cookie, cake, chip, cocoa, chocolate chip\n",
      "\n",
      "Topic  13\n",
      "lemon, sugar, mayonnaise, celery, sprig, blueberry, shrimp, parsley\n",
      "\n",
      "Topic  14\n",
      "confectioner sugar, confectioner, sugar, butter, vanilla, vanilla extract, flour, peppermint\n",
      "\n",
      "Topic  15\n",
      "egg, flour, sugar, bacon, vanilla, vanilla extract, butter, maple\n",
      "\n",
      "Topic  16\n",
      "onion, tomato, lime, bean, jalapeno, jalapeno pepper, green onion, pepper\n",
      "\n",
      "Topic  17\n",
      "worcestershire sauce, worcestershire, beef, broth, garlic, beef broth, onion, bread\n",
      "\n",
      "Topic  18\n",
      "jack, jack cheese, monterey jack, monterey, cheese, tortilla, enchilada, enchilada sauce\n",
      "\n",
      "Topic  19\n",
      "red, red pepper, vinegar, honey, soy sauce, soy, balsamic, balsamic vinegar\n",
      "\n",
      "Topic  20\n",
      "apple, cider, apple cider, cider vinegar, cinnamon, mustard, vinegar, dijon\n",
      "\n",
      "Topic  21\n",
      "buttermilk, flour, buttermilk biscuit, butter, sugar, food, vinegar, cornmeal\n",
      "\n",
      "Topic  22\n",
      "cheese, parmesan, parmesan cheese, mozzarella cheese, mozzarella, basil, pizza, spinach\n",
      "\n",
      "Topic  23\n",
      "coconut, coconut oil, pecan, oil, pineapple, honey, oat, banana\n",
      "\n",
      "Topic  24\n",
      "potato, butter, red potato, milk, yukon gold, yukon, gold potato, gold\n",
      "\n",
      "Topic  25\n",
      "sea, sea salt, flour, yeast, butter, nut, chocolate, oat\n",
      "\n",
      "Topic  26\n",
      "orange, juice, cranberry, orange juice, pineapple, zest, jalapeno, jalapeno pepper\n",
      "\n",
      "Topic  27\n",
      "pie, crust, pie crust, corn, corn starch, starch, pumpkin, butter\n",
      "\n",
      "Topic  28\n",
      "cracker, graham, graham cracker, lime, butter, condensed milk, pretzel, pecan\n",
      "\n",
      "Topic  29\n",
      "bbq sauce, bbq, pork, bacon, pickle, grape, dill pickle, dill\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Experiment with different numbers of topics\n",
    "n_topics = [15, 20, 25, 30]\n",
    "best_nmf_model = None\n",
    "best_score = float('inf')\n",
    "\n",
    "for n in n_topics:\n",
    "    nmf_model = NMF(n, random_state=10, max_iter=2000)\n",
    "    doc_topic = nmf_model.fit_transform(doc_word)\n",
    "    topic_word = nmf_model.components_\n",
    "    score = nmf_model.reconstruction_err_\n",
    "    \n",
    "    if score < best_score:\n",
    "        best_score = score\n",
    "        best_nmf_model = nmf_model\n",
    "\n",
    "nmf_model = best_nmf_model\n",
    "doc_topic = nmf_model.fit_transform(doc_word)\n",
    "topic_word = nmf_model.components_\n",
    "\n",
    "def display_topics(model, feature_names, num_top_words, topic_names=None):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[idx]:\n",
    "            print(\"\\nTopic \", idx)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\", topic_names[idx], \"'\")\n",
    "        print(\", \".join([feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]))\n",
    "\n",
    "display_topics(nmf_model, vectorizer.get_feature_names_out(), 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "key_ingredients_weights = {\n",
    "    'beef': 10,\n",
    "    'chicken': 10,\n",
    "    'shrimp': 10,\n",
    "    'crab': 10,\n",
    "    'venison': 10,\n",
    "    # Add more key ingredients and their specific weights\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input:  egg\n",
      "Tokens Generated:  egg \n",
      "\n",
      "OVEN AND AIR FRYER HARD COOKED EGGS\n",
      "1 egg \n",
      "\n",
      "MERINGUE RECIPE\n",
      "4 large egg whites, 1/4 teaspoon cream of tartar, 1/4 cup sugar \n",
      "\n",
      "ULTIMATE KALE SALAD RECIPE\n",
      "4 cups kale greens (trimmed, chopped and massaged), 2 cups spring lettuce mix, 8 roasted radishes, 2 cucumber (peeled and chopped), 8 okra pods (halved lengthwise), 4 boiled eggs (halved), 2 avocado (halved), salt and pepper (to taste), dressing of choice \n",
      "\n",
      "SALMON EGGS BENEDICT\n",
      "8 - 9 ounces spinach (steamed), salmon steaks (broiled), 4 eggs (poached), hollandaise sauce, salt and pepper \n",
      "\n",
      "CLASSIC EGG SALAD RECIPE\n",
      "6 large hard cooked eggs, Â¼ cup mayonnaise, salt and pepper (to taste) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "useringreds = \"egg\"\n",
    "usertokens = user_tokenize(useringreds)\n",
    "print('User Input: ', useringreds)\n",
    "print('Tokens Generated: ', usertokens, '\\n')\n",
    "\n",
    "# Vectorize user input\n",
    "user_vec = vectorizer.transform([usertokens])\n",
    "\n",
    "# Adjust weights for key ingredients\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for ingredient, weight in key_ingredients_weights.items():\n",
    "    if ingredient in usertokens:\n",
    "        index = feature_names.tolist().index(ingredient)\n",
    "        user_vec[0, index] *= weight\n",
    "\n",
    "# Transform user vector into topic space\n",
    "topic_vec = nmf_model.transform(user_vec)\n",
    "\n",
    "# Compute similarity and get recommendations\n",
    "indices = pairwise_distances(topic_vec, doc_topic, metric='cosine').argsort().ravel()\n",
    "\n",
    "for index in indices[0:5]:\n",
    "    print(df.iloc[index].Title.upper())\n",
    "    print(df.iloc[index][\"All Ingredients\"], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpan NMF model\n",
    "with open('nmf_model.pkl', 'wb') as f:\n",
    "    pickle.dump(nmf_model, f)\n",
    "\n",
    "# Simpan vectorizer\n",
    "with open('vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "# Simpan data doc_topic\n",
    "with open('doc_topic.pkl', 'wb') as f:\n",
    "    pickle.dump(doc_topic, f)\n",
    "\n",
    "# Simpan data frame df_recipes\n",
    "with open('df.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorizer.pkl to json\n",
    "\n",
    "\n",
    "# Ekstraksi atribut-atribut yang relevan\n",
    "vectorizer_data = {\n",
    "    'vocabulary_': vectorizer.vocabulary_,\n",
    "    'idf_': vectorizer.idf_.tolist(),\n",
    "    'tokenizer': 'commatokenizer',  # Simpan nama tokenizer sebagai referensi\n",
    "}\n",
    "\n",
    "# Simpan ke file JSON\n",
    "with open('vectorizer.json', 'w') as outfile:\n",
    "    json.dump(vectorizer_data, outfile, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
